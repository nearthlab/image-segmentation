# Training settings
[GENERAL]
    # Number of training steps per epoch
    # This doesn't need to match the size of the training set. Tensorboard
    # updates are saved at the end of each epoch, so setting this to a
    # smaller number means getting more frequent TensorBoard updates.
    # Validation stats are also calculated at each epoch end and they
    # might take a while, so don't set this too small to avoid spending
    # a lot of time on validation stats.
    int-STEPS_PER_EPOCH = 1000

    # Number of total training epochs
    int-EPOCHS = 40

    # Number of validation steps to run at the end of every training epoch.
    # A bigger number improves accuracy of validation stats, but slows
    # down the training.
    int-VALIDATION_STEPS = 20

    # Learning rate and momentum
    float-LEARNING_RATE = 0.001
    float-LEARNING_MOMENTUM = 0.9

[LAYERS]
    # Train or freeze batch normalization layers
    bool-TRAIN_BN = False

    # Trainable layers
    list-TRAINABLE_LAYERS = ["fpn", "rpn", "mrcnn"]

[LOSS]
    # Weight decay for l2 regularization
	# Set this value so that regularization loss is about in the similar range with other losses
    float-WEIGHT_DECAY = 0.5

    # Loss weights for more precise optimization.
    dict-LOSS_WEIGHTS = {"rpn_class_loss": 1.0, "rpn_bbox_loss": 1.0, "mrcnn_class_loss": 1.0, "mrcnn_bbox_loss": 1.0, "mrcnn_mask_loss": 1.0}

[OPTIMIZER]
    # Optimizer
    # Possible values:
    str-OPTIMIZER = SGD

    # Gradient norm clipping
    float-GRADIENT_CLIP_NORM = 5.0

[SGD]
    # Whether to use nesterov
    # This is going to be used only when OPTIMIZER is SGD
    # Otherwise this value is meaningless
    bool-NESTEROV = True

[Adam]
    # Whether to use amsgrad
    # This is going to be used only when OPTIMIZER is Adam
    # Otherwise this value is meaningless
    bool-AMSGRAD = True

